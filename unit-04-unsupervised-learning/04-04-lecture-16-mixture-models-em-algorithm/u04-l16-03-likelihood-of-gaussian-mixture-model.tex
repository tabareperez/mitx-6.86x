% Created 2020-04-22 mié 21:04
% Intended LaTeX compiler: pdflatex
\documentclass[a4paper, 12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{float, amsfonts, commath, mathtools, proba}
\author{Tabaré Pérez}
\date{\today}
\title{Lecture 16 - 3: Likelihood of Gaussian Mixture Model}
\hypersetup{
 pdfauthor={Tabaré Pérez},
 pdftitle={Lecture 16 - 3: Likelihood of Gaussian Mixture Model},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.3.6)}, 
 pdflang={English}}
\begin{document}

\maketitle
\begin{equation}
\label{eq:orgbe71e82}
\theta; \prob_1 \ldots \prob_K, \mu^{(1)} \ldots \mu^{(K)}, \sigma_{1}^{2} \ldots \sigma_{K}^{2}
\end{equation}

So as you see, there's a whole family of these parameters that we need to
determine. But before, we're going to start having a conversation of how,
actually, to compute these \(\theta\)s I just want to show you yet another
piece.

How can we compute the likelihood of a particular point to be generated given
these parameters?

And if you look here, I ask this question in the case of a single Gaussian.

When we have our parameters and we ask: what is the likelihood of \(x\)?

So here, remember that when I was writing x and theta there is actually the
whole family of these parameters, How can we do it?

And now you remember that the point may be generated from various different
clusters. We don't know which one contributed to it. So what we will do, we will
now marginalize over cluster assignment, meaning we would look at the likelihood
of it being generated by all possible clusters, and then we will sum it up:

\begin{equation}
\label{eq:orgd694d46}
\prob(x|\theta) = \sum_{j=1}^{K} \prob_j \mathcal{N}(x, \mu^{(j)}, \sigma_{j}^{2})
\end{equation}

So in this case, we are going to go from \(1\) to \(K\)
and then look at the sum of the likelihoods of the point being
generated for each one of the clusters.

So now we have everything we need
to think about a mixture model.

And the next question for us is how we can actually
find all these different paths, all these different parameters,
given a specific assignment of points on the plane.

And you can see that there is a lot of dependency here
because, you know, in general case,
we don't really know from where all these points come.
We don't know to which class they belong.
So that makes the matter much more complex.

So let me first write you down the likelihood. We will do exactly the same thing
as we've done before. We said OK, I'm giving you some set of points, \(S_n\):

\begin{equation}
\label{eq:orgd16629e}
\prob(S_n|\theta) = \prod_{i=1}^{n} \sum_{j=1}^{K} \prob_j \mathcal{N}(x, \mu^{(j)}, \sigma_{j}^{2} I) 
\end{equation}

It has \(n\) points with parameter \(\theta\). And what, in fact, it means? That
we need to go and multiply, because all those points are independent.

We're going to go from point \(1\) to point \(n\) and multiply its likelihood to
be generated by this Gaussian.

So now given this expression, I want to take the derivative of it with respect
to my parameters, make it equal to 0, and find the parameters.

It turns out, it's actually a pretty complex task. So what we will do now, we
will start with an easy case.

The first case that I will consider with you is the case which is called
observed. I would assume that somebody gave me this point, but they actually
gave me the hard assignment. They said, this point belongs to the first cluster,
and this point belongs to the second cluster. You're going to measure them.
Then, you know, if we would have here red and blue, that some of them are red
and some of them are blue. So we have no uncertainty. We know your belonging.
And I will demonstrate to you how we can solve that piece. And if we understand
it well, we can very easily translate it into the case where the assignment is
not observed. And that's where we are going to be talking about \textbf{E-M ALGORITHM}.

\textbf{NOTES}

\begin{itemize}
\item What is the difference between "mixture components" and Clusters ?
\item What is the difference between "mixture weights" and probability?
\end{itemize}

You can consider them as synonyms although they are not quiet. Here is my
understanding:

\begin{itemize}
\item The number of mixture components is the true number that underlies the
generation of the data (the data is an output), while the number of clusters
(in the K-means and K-medoids algorithms) is the size of the partition you
want your algorithm to generate from the data (the data is an input). When
analyzing the data, often we won't have prior knowledge of what the real
number of mixture components is. We'll have to either guess it or and try to
estimate it by iterating over different numbers of clusters and comparing
their outcomes.
\item "Mixture weights" are probabilities if the number of components is finite. If
we were to face a non-discrete set of mixture components, then we would need
to use the probability density terminology.
\end{itemize}
\end{document}